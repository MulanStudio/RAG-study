import os
import sys

# æ£€æŸ¥ OpenAI API Key
if "OPENAI_API_KEY" not in os.environ:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡ã€‚")
    print("è¯·å…ˆè®¾ç½®æ‚¨çš„ API Keyï¼Œä¾‹å¦‚: export OPENAI_API_KEY='sk-...'")
    print("ä¸ºäº†æ¼”ç¤ºä»£ç é€»è¾‘ï¼Œç¨‹åºå°†ç»§ç»­è¿è¡Œï¼Œä½†åœ¨è°ƒç”¨ LLM æ—¶å¯èƒ½ä¼šå¤±è´¥ã€‚\n")

try:
    from langchain_community.document_loaders import (
        TextLoader, 
        DirectoryLoader, 
        PyPDFLoader, 
        UnstructuredExcelLoader,
        Docx2txtLoader
    )
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    # from langchain_openai import OpenAIEmbeddings, ChatOpenAI
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_chroma import Chroma
    # ç›´æ¥å¼•å…¥ sentence-transformers çš„åŸç”Ÿ CrossEncoder
    from sentence_transformers import CrossEncoder
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.runnables import RunnablePassthrough
    from langchain_core.output_parsers import StrOutputParser
except ImportError as e:
    print(f"ç¼ºå°‘å¿…è¦çš„åº“: {e}")
    print("è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š")
    print("pip install langchain langchain-community langchain-huggingface langchain-chroma chromadb sentence-transformers pypdf openpyxl docx2txt unstructured rank_bm25")
    sys.exit(1)

def run_rag_demo():
    print("--- 1. Indexing (ç´¢å¼•é˜¶æ®µ) ---")
    
    # 1.1 Load (åŠ è½½æ–‡æ¡£) - æ”¯æŒå¤šæ ¼å¼
    print("æ­£åœ¨åŠ è½½ knowledge_base/ å’Œ downloads/ ç›®å½•ä¸‹çš„æ–‡æ¡£...")
    
    loaders = [
        # åŠ è½½ Markdown
        DirectoryLoader("knowledge_base", glob="**/*.md", loader_cls=TextLoader),
        # åŠ è½½ä¸‹è½½çš„ PDF
        DirectoryLoader("downloads", glob="**/*.pdf", loader_cls=PyPDFLoader),
        # åŠ è½½ Excel
        DirectoryLoader("downloads", glob="**/*.xlsx", loader_cls=UnstructuredExcelLoader),
        # åŠ è½½ Word
        DirectoryLoader("downloads", glob="**/*.docx", loader_cls=Docx2txtLoader),
    ]
    
    docs = []
    for loader in loaders:
        try:
            loaded_docs = loader.load()
            docs.extend(loaded_docs)
            print(f"âœ… æˆåŠŸåŠ è½½: {loader.glob} (æ•°é‡: {len(loaded_docs)})")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å¤±è´¥ {loader.glob}: {e}")

    if not docs:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ç›®å½•ã€‚")
        return

    print(f"æ€»è®¡åŠ è½½æ–‡æ¡£æ•°: {len(docs)}")
    print(f"æ–‡æ¡£æ€»å­—ç¬¦æ•°: {sum(len(d.page_content) for d in docs)}")

    # 1.2 Split (æ–‡æ¡£åˆ‡åˆ†)
    # ä½¿ç”¨ RecursiveCharacterTextSplitter æ™ºèƒ½åˆ‡åˆ†ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
    print("æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, 
        chunk_overlap=50
    )
    splits = text_splitter.split_documents(docs)
    print(f"æ–‡æ¡£å·²åˆ‡åˆ†ä¸º {len(splits)} ä¸ªç‰‡æ®µ (Chunks)ã€‚")

    # 1.3 Embed & Store (å‘é‡åŒ–ä¸å­˜å‚¨)
    # å°†æ–‡æœ¬ç‰‡æ®µè½¬æ¢ä¸ºå‘é‡ï¼Œå¹¶å­˜å…¥ Chroma å‘é‡æ•°æ®åº“
    print("æ­£åœ¨è¿›è¡Œå‘é‡åŒ–å¹¶å­˜å…¥ ChromaDB (ä½¿ç”¨å…è´¹çš„ HuggingFace æ¨¡å‹)...")
    try:
        # ä½¿ç”¨æœ¬åœ° CPU è¿è¡Œçš„å…è´¹æ¨¡å‹ï¼Œä¸éœ€è¦ API Key
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vectorstore = Chroma.from_documents(
            documents=splits, 
            embedding=embeddings
        )
        
        # 1.4 åŠ è½½ Reranker æ¨¡å‹
        print("æ­£åœ¨åŠ è½½ Reranker æ¨¡å‹ (BAAI/bge-reranker-base)...")
        # ç›´æ¥ä½¿ç”¨ CrossEncoderï¼Œä¸ä¾èµ– LangChain å°è£…
        reranker = CrossEncoder("BAAI/bge-reranker-base")
        print("å‘é‡æ•°æ®åº“ä¸é‡æ’åºå™¨ (Reranker) æ„å»ºå®Œæˆã€‚")
            
    except Exception as e:
        print(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")
        return

    print("\n--- 2. Retrieval Only (ä»…æ¼”ç¤ºæ£€ç´¢é˜¶æ®µ) ---")
    print("ç”±äºæ²¡æœ‰æœ¬åœ°å¤§æ¨¡å‹ (LLM)ï¼Œæˆ‘ä»¬å°†ç›´æ¥å±•ç¤º RAG æ£€ç´¢åˆ°çš„çŸ¥è¯†ç‰‡æ®µã€‚")
    print("è¿™ä¸€æ­¥æ˜¯ RAG æˆåŠŸçš„å…³é”®ï¼šå¦‚æœæ‰¾åˆ°äº†æ­£ç¡®ç‰‡æ®µï¼ŒLLM åªéœ€è¦æŠŠå®ƒä»¬æ¶¦è‰²ä¸€ä¸‹ã€‚")

    # 2.4 Invoke (æ‰§è¡ŒæŸ¥è¯¢)
    questions = [
        "å…¨çƒæœ€å¤§çš„æ²¹æœå…¬å¸æ˜¯è°ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯æ°´åŠ›å‹è£‚ï¼ˆFrackingï¼‰ï¼Ÿå®ƒæœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ",
        "Schlumberger çš„ 2023 å¹´è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ(è¯·æŸ¥æ‰¾ Excel æ•°æ®)",
        "åˆåŒä¸­è§„å®šçš„é’»äº•æ—¥è´¹ç‡(day rate)æ˜¯å¤šå°‘ï¼Ÿ(è¯·æŸ¥æ‰¾ Word åˆåŒ)",
        "ä¸­æµ·æ²¹æœ(COSL)çš„ä¸šåŠ¡æ¦‚å†µæ˜¯ä»€ä¹ˆï¼Ÿ(è¯·æŸ¥æ‰¾ PDF å¹´æŠ¥)",
    ]

    for q in questions:
        print(f"\n" + "="*40)
        print(f"ç”¨æˆ·æé—®: {q}")
        print("-" * 40)
        
        # 1. ç¬¬ä¸€æ­¥ï¼šç²—æ’ (å¬å› Top 20)
        print("ğŸ” 1. åˆæ­¥æ£€ç´¢ (Recall Top 20)...")
        initial_docs = vectorstore.similarity_search(q, k=20)
        
        # 2. ç¬¬äºŒæ­¥ï¼šç²¾æ’ (Rerank)
        print("ğŸ” 2. é‡æ’åº (Reranking)...")
        # æ„é€  input pairs: [[query, doc_text1], [query, doc_text2], ...]
        pairs = [[q, doc.page_content] for doc in initial_docs]
        
        # è®¡ç®—åˆ†æ•°
        scores = reranker.predict(pairs)
        
        # å°†æ–‡æ¡£å’Œåˆ†æ•°æ‰“åŒ…ï¼Œå¹¶æŒ‰åˆ†æ•°é™åºæ’åˆ—
        doc_score_pairs = list(zip(initial_docs, scores))
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
        
        # å– Top 3
        top_k_docs = doc_score_pairs[:3]
        
        for i, (doc, score) in enumerate(top_k_docs):
            print(f"\n[æ’å #{i+1} | ç›¸å…³æ€§å¾—åˆ†: {score:.4f}]:")
            # æ‰“å°æºæ–‡ä»¶å (å¦‚æœæœ‰å…ƒæ•°æ®)
            source = doc.metadata.get('source', 'unknown')
            print(f"æ¥æº: {source}")
            print(f"å†…å®¹: {doc.page_content[:300].replace(chr(10), ' ')}...") # æ›¿æ¢æ¢è¡Œç¬¦ä»¥ä¾¿å±•ç¤º
            
    print("\n" + "="*40)
    print("âœ… æ¼”ç¤ºç»“æŸã€‚")
    print("åŸç†è§£é‡Š: æˆ‘ä»¬æˆåŠŸæ‰¾åˆ°äº†é—®é¢˜çš„ç­”æ¡ˆæ‰€åœ¨ä½ç½®ã€‚")
    print("å¦‚æœæ­¤æ—¶æ¥å…¥ä¸€ä¸ª LLM (å¦‚ GPT-4 æˆ–æœ¬åœ° Ollama)ï¼Œå®ƒå°±ä¼šé˜…è¯»ä¸Šè¿°ç‰‡æ®µå¹¶è¾“å‡ºé€šé¡ºçš„è‡ªç„¶è¯­è¨€å›ç­”ã€‚")

if __name__ == "__main__":
    run_rag_demo()

