#!/usr/bin/env python3
"""
ğŸ›¢ï¸ æ²¹ç”°æœåŠ¡ RAG ç³»ç»Ÿ - ä¸»åº”ç”¨å…¥å£

ä½¿ç”¨æ–¹æ³•:
    1. æŠŠç»„å§”ä¼šæ•°æ®æ”¾åˆ° data/ æ–‡ä»¶å¤¹
    2. è¿è¡Œ: python app.py

æˆ–è€…æŒ‡å®šæ•°æ®ç›®å½•:
    python app.py --data_dir /path/to/data

å¯åŠ¨ Web UI:
    python app.py --mode web
"""

import os
import sys
import argparse
import uuid
import yaml
import logging
from typing import List, Dict

# è®¾ç½®ç¯å¢ƒ
os.environ["NO_PROXY"] = "localhost,127.0.0.1"

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ·»åŠ è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, "src"))


def load_config(config_path: str = "config/config.yaml") -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    default_config = {
        "data": {"root_dir": "data/"},
        "models": {
            "llm": {"provider": "azure_openai", "model_name": "gpt-5-chat", "base_url": ""},
            "embedding": {"provider": "azure_openai", "model_name": "text-embedding-3-large"},
            "reranker": {"model_name": "cross-encoder/ms-marco-MiniLM-L-6-v2"}
        },
        "indexing": {
            "chunk_size_parent": 2000,
            "chunk_overlap_parent": 200,
            "chunk_size_child": 400,
            "chunk_overlap_child": 50
        }
    }
    
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            # åˆå¹¶é»˜è®¤é…ç½®
            for key in default_config:
                if key not in config:
                    config[key] = default_config[key]
            return config
    
    return default_config


class OilfieldRAG:
    """
    æ²¹ç”°æœåŠ¡ RAG ç³»ç»Ÿä¸»ç±»
    
    Example:
        rag = OilfieldRAG(data_dir="data/")
        answer = rag.ask("What is the revenue of SLB?")
    """
    
    def __init__(self, data_dir: str = "data/", config_path: str = "config/config.yaml"):
        self.data_dir = data_dir
        self.config = load_config(config_path)
        self.config_path = config_path
        
        self.vectorstore = None
        self.bm25 = None
        self.reranker = None
        self.llm = None
        self.retriever = None
        self.generator = None
        self.docstore = {}
        self.splits = []
        
        self._initialized = False
    
    def initialize(self, verbose: bool = True):
        """åˆå§‹åŒ–ç³»ç»Ÿï¼ˆåŠ è½½æ•°æ®ã€æ„å»ºç´¢å¼•ï¼‰"""
        if self._initialized:
            print("âš ï¸ ç³»ç»Ÿå·²åˆå§‹åŒ–")
            return
        
        print("=" * 60)
        print("ğŸ›¢ï¸ æ²¹ç”°æœåŠ¡ RAG ç³»ç»Ÿ åˆå§‹åŒ–")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        print("\nğŸ“‚ Step 1: åŠ è½½æ•°æ®...")
        from src.member_a_data.loaders import load_all_documents
        
        # æ£€æŸ¥ VLM
        vlm = self._init_vlm()
        
        docs = load_all_documents(self.data_dir, vlm=vlm, verbose=verbose)
        
        if not docs:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•")
            return
        
        # 1.5. é¢„å¤„ç†ï¼šå…ƒæ•°æ®æ¸…æ´— + æ‘˜è¦ç”Ÿæˆ
        docs = self._preprocess_documents(docs, verbose=verbose)
        
        # 2. æ–‡æ¡£åˆ‡åˆ† (Parent-Child)
        print("\nğŸ“‘ Step 2: æ–‡æ¡£åˆ‡åˆ†...")
        self.splits, self.docstore = self._split_documents(docs)
        print(f"   ç”Ÿæˆ {len(self.splits)} ä¸ªå­å—")
        
        # 3. æ„å»ºå‘é‡ç´¢å¼•
        print("\nğŸ” Step 3: æ„å»ºå‘é‡ç´¢å¼•...")
        self.vectorstore = self._build_vectorstore(self.splits)
        
        # 4. æ„å»º BM25 ç´¢å¼•
        print("\nğŸ“ Step 4: æ„å»º BM25 ç´¢å¼•...")
        self.bm25 = self._build_bm25(self.splits)
        
        # 5. åŠ è½½ Reranker
        print("\nğŸ¯ Step 5: åŠ è½½ Reranker...")
        self.reranker = self._load_reranker()
        
        # 6. åŠ è½½ LLM
        print("\nğŸ¤– Step 6: è¿æ¥ LLM...")
        self.llm = self._init_llm()
        
        # 7. åˆå§‹åŒ–æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨
        print("\nâš™ï¸ Step 7: åˆå§‹åŒ–æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨...")
        from src.member_b_retrieval.retrieval import RAGRetriever
        from src.member_c_generation.generation import create_generator
        
        self.retriever = RAGRetriever(
            vectorstore=self.vectorstore,
            bm25=self.bm25,
            splits=self.splits,
            reranker=self.reranker,
            docstore=self.docstore,
            llm=self.llm,
            config=self.config
        )
        
        self.generator = create_generator(self.llm, self.config_path)
        
        self._initialized = True
        
        print("\n" + "=" * 60)
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ!")
        print(f"   æ–‡æ¡£æ•°: {len(docs)}")
        print(f"   ç´¢å¼•å—: {len(self.splits)}")
        print(f"   LLM: {'åœ¨çº¿' if self.llm else 'ç¦»çº¿'}")
        print("=" * 60)
    
    def ask(self, question: str, verbose: bool = False) -> str:
        """
        é—®ç­”æ¥å£
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            verbose: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯
        
        Returns:
            ç­”æ¡ˆå­—ç¬¦ä¸²
        """
        if not self._initialized:
            self.initialize()
        
        if verbose:
            print(f"\nâ“ é—®é¢˜: {question}")
        
        # æ£€ç´¢
        docs, retrieval_debug = self.retriever.retrieve(question, top_k=5)
        retrieval_score = retrieval_debug.get("max_similarity_score", 1.0)
        core_query = retrieval_debug.get("core_query", question)
        
        if verbose:
            print(f"ğŸ“š æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£ (ç›¸ä¼¼åº¦: {retrieval_score:.2f})")
            if core_query != question:
                print(f"   ğŸ’¡ æ ¸å¿ƒé—®é¢˜: {core_query}")
            for i, doc in enumerate(docs[:3], 1):
                print(f"   {i}. {doc.page_content[:80]}...")
        
        # ç”Ÿæˆï¼ˆä¼ å…¥æ£€ç´¢åˆ†æ•°å’Œæ ¸å¿ƒé—®é¢˜ç”¨äºç½®ä¿¡åº¦åˆ¤æ–­å’Œå¯¹é½æ£€æŸ¥ï¼‰
        answer, gen_debug = self.generator.generate(
            question, docs, 
            retrieval_score=retrieval_score,
            core_query=core_query
        )
        
        if verbose:
            print(f"\nğŸ’¬ ç­”æ¡ˆ: {answer[:200]}...")
        
        return answer
    
    def _preprocess_documents(self, docs: List, verbose: bool = True) -> List:
        """
        é¢„å¤„ç†æ–‡æ¡£ï¼šå…ƒæ•°æ®æ¸…æ´— + æ‘˜è¦ç”Ÿæˆ
        
        Args:
            docs: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            verbose: æ˜¯å¦æ‰“å°è¿›åº¦
            
        Returns:
            é¢„å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨
        """
        preprocess_cfg = self.config.get("preprocessing", {})
        
        # 1. å…ƒæ•°æ®æ¸…æ´—
        if preprocess_cfg.get("enable_metadata_cleaning", True):
            print("\nğŸ§¹ Step 1.5a: å…ƒæ•°æ®æ¸…æ´—...")
            from src.member_a_data.metadata_cleaner import clean_metadata
            docs = clean_metadata(docs, verbose=verbose)
            print(f"   æ¸…æ´—å®Œæˆï¼š{len(docs)} ä¸ªæ–‡æ¡£")
        
        # 2. æ–‡æœ¬å—æ‘˜è¦ï¼ˆå¯é€‰ï¼Œä¾èµ– LLMï¼‰
        if preprocess_cfg.get("enable_summarization", False):
            print("\nğŸ“ Step 1.5b: ç”Ÿæˆæ–‡æœ¬å—æ‘˜è¦...")
            
            # åˆå§‹åŒ– LLMï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
            if not self.llm:
                self.llm = self._init_llm()
            
            # è·å–æ‘˜è¦é…ç½®
            sum_cfg = preprocess_cfg.get("summarization", {})
            
            from src.member_a_data.chunk_summarizer import CachedChunkSummarizer
            
            # ä»é…ç½®è¯»å– prompts
            prompts = sum_cfg.get("prompts", None)
            
            summarizer = CachedChunkSummarizer(
                llm=self.llm,
                prompts=prompts,
                min_length=sum_cfg.get("min_length", 300),
                max_input_length=sum_cfg.get("max_input_length", 3000),
                prepend_summary=sum_cfg.get("prepend_summary", True),
                cache_dir=sum_cfg.get("cache_dir", ".summary_cache")
            )
            
            docs = summarizer.summarize(docs, verbose=verbose)
            print(f"   æ‘˜è¦ç”Ÿæˆå®Œæˆï¼š{len(docs)} ä¸ªæ–‡æ¡£")
        
        return docs
    
    def _split_documents(self, docs: List) -> tuple:
        """Parent-Child åˆ‡åˆ†"""
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        
        cfg = self.config["indexing"]
        parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=cfg["chunk_size_parent"],
            chunk_overlap=cfg["chunk_overlap_parent"]
        )
        child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=cfg["chunk_size_child"],
            chunk_overlap=cfg["chunk_overlap_child"]
        )
        
        docstore = {}
        child_docs = []
        
        for raw_doc in docs:
            # åˆ‡åˆ†ä¸º Parent
            if len(raw_doc.page_content) > cfg["chunk_size_parent"]:
                parents = parent_splitter.split_documents([raw_doc])
            else:
                parents = [raw_doc]
            
            for parent in parents:
                parent_id = str(uuid.uuid4())
                parent.metadata["doc_id"] = parent_id
                docstore[parent_id] = parent
                
                # åˆ‡åˆ†ä¸º Child
                children = child_splitter.split_documents([parent])
                for child in children:
                    child.metadata["parent_id"] = parent_id
                    child_docs.append(child)
        
        return child_docs, docstore
    
    def _compute_data_fingerprint(self, docs: List) -> str:
        """è®¡ç®—æ•°æ®æŒ‡çº¹ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•"""
        import hashlib
        # ä½¿ç”¨æ–‡æ¡£æ•°é‡ + å‰100ä¸ªæ–‡æ¡£çš„å‰100å­—ç¬¦ä½œä¸ºæŒ‡çº¹
        fingerprint_data = f"{len(docs)}:"
        for doc in docs[:100]:
            fingerprint_data += doc.page_content[:100]
        return hashlib.md5(fingerprint_data.encode()).hexdigest()[:16]
    
    def _get_embeddings(self):
        """è·å– embedding æ¨¡å‹å®ä¾‹"""
        emb_cfg = self.config["models"]["embedding"]
        provider = emb_cfg.get("provider", "huggingface")
        
        if provider == "azure_openai":
            from src.member_e_system.azure_openai_client import (
                create_azure_openai_client,
                AzureOpenAIEmbeddings,
                load_azure_settings,
            )
            azure_cfg = load_azure_settings(self.config)
            client = create_azure_openai_client(
                azure_cfg["team_domain"],
                azure_cfg["api_key"]
            )
            return AzureOpenAIEmbeddings(client, azure_cfg["embedding_model"])
        elif provider == "huggingface_local":
            # æœ¬åœ° GPU åŠ é€Ÿæ¨¡å‹
            from langchain_huggingface import HuggingFaceEmbeddings
            model_name = emb_cfg.get("model_name", "BAAI/bge-large-en-v1.5")
            device = emb_cfg.get("device", "cuda")
            return HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={'device': device},
                encode_kwargs={'batch_size': 256, 'normalize_embeddings': True}
            )
        else:
            from langchain_huggingface import HuggingFaceEmbeddings
            model_name = emb_cfg["model_name"]
            return HuggingFaceEmbeddings(model_name=model_name)
    
    def _build_vectorstore(self, docs: List):
        """æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆæ”¯æŒæŒä¹…åŒ–ç¼“å­˜ï¼‰"""
        from langchain_chroma import Chroma
        import time
        
        # è¿‡æ»¤ç©ºå†…å®¹ï¼Œé¿å… embedding æŠ¥é”™
        docs = [doc for doc in docs if doc.page_content and doc.page_content.strip()]
        print(f"   ğŸ“¦ å‘é‡æ„å»ºè¾“å…¥æ–‡æ¡£æ•°: {len(docs)}")
        
        # è·å–æŒä¹…åŒ–é…ç½®
        sys_cfg = self.config.get("system", {}).get("vector_db", {})
        persist_dir = sys_cfg.get("persist_dir", ".cache/chroma_db")
        force_rebuild = sys_cfg.get("force_rebuild", False)
        
        # è®¡ç®—æ•°æ®æŒ‡çº¹
        data_fingerprint = self._compute_data_fingerprint(docs)
        hash_file = os.path.join(persist_dir, "data_fingerprint.txt")
        
        # è·å– embedding æ¨¡å‹
        embeddings = self._get_embeddings()
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä»ç¼“å­˜åŠ è½½
        if os.path.exists(persist_dir) and not force_rebuild:
            if os.path.exists(hash_file):
                with open(hash_file, 'r') as f:
                    cached_fingerprint = f.read().strip()
                if cached_fingerprint == data_fingerprint:
                    print("   ğŸ’¾ ä»ç¼“å­˜åŠ è½½å‘é‡ç´¢å¼•...")
                    start = time.time()
                    vectorstore = Chroma(
                        persist_directory=persist_dir,
                        embedding_function=embeddings
                    )
                    elapsed = time.time() - start
                    print(f"   âœ… ç¼“å­˜åŠ è½½å®Œæˆï¼Œç”¨æ—¶ {elapsed:.1f}s")
                    return vectorstore
                else:
                    print("   ğŸ”„ æ•°æ®å·²å˜åŒ–ï¼Œé‡å»ºç´¢å¼•...")
            else:
                print("   ğŸ”„ ç¼“å­˜æ— æŒ‡çº¹æ–‡ä»¶ï¼Œé‡å»ºç´¢å¼•...")
        
        # é‡æ–°æ„å»ºç´¢å¼•
        start = time.time()
        print("   â±ï¸ å¼€å§‹æ„å»º Chroma å‘é‡ç´¢å¼•...")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(persist_dir, exist_ok=True)
        
        vectorstore = Chroma.from_documents(
            documents=docs,
            embedding=embeddings,
            persist_directory=persist_dir
        )
        
        # ä¿å­˜æ•°æ®æŒ‡çº¹
        with open(hash_file, 'w') as f:
            f.write(data_fingerprint)
        
        elapsed = time.time() - start
        print(f"   âœ… å‘é‡ç´¢å¼•å®Œæˆå¹¶æŒä¹…åŒ–ï¼Œç”¨æ—¶ {elapsed:.1f}s")
        return vectorstore
    
    def _build_bm25(self, docs: List):
        """æ„å»º BM25 ç´¢å¼•"""
        from rank_bm25 import BM25Okapi
        from src.member_b_retrieval.text_processing import tokenize_text
        
        tokenized = [tokenize_text(doc.page_content) for doc in docs]
        return BM25Okapi(tokenized)
    
    def _load_reranker(self):
        """åŠ è½½ Reranker"""
        from sentence_transformers import CrossEncoder
        
        model_name = self.config["models"]["reranker"]["model_name"]
        return CrossEncoder(model_name)
    
    def _init_llm(self):
        """åˆå§‹åŒ– LLM"""
        try:
            cfg = self.config["models"]["llm"]
            provider = cfg.get("provider", "ollama")
            if provider == "azure_openai":
                from src.member_e_system.azure_openai_client import create_azure_openai_client, AzureOpenAIChat, load_azure_settings
                azure_cfg = load_azure_settings(self.config)
                client = create_azure_openai_client(
                    azure_cfg["team_domain"],
                    azure_cfg["api_key"]
                )
                model = azure_cfg["completion_model"] or cfg["model_name"]
                llm = AzureOpenAIChat(client, model, temperature=cfg.get("temperature", 0.1))
                llm.invoke("hi")
                print(f"   âœ… LLM è¿æ¥æˆåŠŸ (Azure): {model}")
                return llm
            else:
                from langchain_ollama import ChatOllama
                llm = ChatOllama(
                    model=cfg["model_name"],
                    base_url=cfg["base_url"]
                )
                llm.invoke("hi")  # æµ‹è¯•è¿æ¥
                print(f"   âœ… LLM è¿æ¥æˆåŠŸ: {cfg['model_name']}")
                return llm
        except Exception as e:
            print(f"   âš ï¸ LLM è¿æ¥å¤±è´¥: {e}")
            return None
    
    def _init_vlm(self):
        """åˆå§‹åŒ– VLM (å›¾ç‰‡ç†è§£)"""
        if not self.config.get("models", {}).get("vlm", {}).get("enabled", False):
            return None
        
        try:
            from langchain_ollama import ChatOllama
            
            cfg = self.config["models"]["vlm"]
            vlm = ChatOllama(
                model=cfg["model_name"],
                base_url=cfg["base_url"]
            )
            vlm.invoke("hi")
            print(f"   âœ… VLM å¯ç”¨: {cfg['model_name']}")
            return vlm
        except Exception as e:
            print(f"   âš ï¸ VLM ä¸å¯ç”¨ï¼Œå›¾ç‰‡å°†ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæè¿° ({type(e).__name__})")
            return None


def run_cli(rag: OilfieldRAG):
    """å‘½ä»¤è¡Œäº¤äº’æ¨¡å¼"""
    print("\nğŸ® è¿›å…¥äº¤äº’æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
    print("-" * 40)
    
    while True:
        try:
            question = input("\nâ“ è¯·è¾“å…¥é—®é¢˜: ").strip()
            
            if question.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§!")
                break
            
            if not question:
                continue
            
            answer = rag.ask(question, verbose=True)
            print(f"\nğŸ’¬ ç­”æ¡ˆ:\n{answer}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break


def run_web(rag: OilfieldRAG):
    """å¯åŠ¨ Web UIï¼ˆå·²å¼ƒç”¨ï¼‰"""
    print("\nâš ï¸ Web UI æ¨¡å¼å·²å¼ƒç”¨")
    print("   è¯·ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼: python src/member_e_system/app.py --question 'ä½ çš„é—®é¢˜'")
    print("   æˆ–äº¤äº’æ¨¡å¼: python src/member_e_system/app.py")
    sys.exit(1)


def main():
    parser = argparse.ArgumentParser(description="æ²¹ç”°æœåŠ¡ RAG ç³»ç»Ÿ")
    parser.add_argument("--data_dir", type=str, default="data/", help="æ•°æ®ç›®å½•")
    parser.add_argument("--config", type=str, default="config/config.yaml", help="é…ç½®æ–‡ä»¶")
    parser.add_argument("--mode", type=str, choices=["cli", "web"], default="cli", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--question", type=str, help="ç›´æ¥æé—®ï¼ˆéäº¤äº’æ¨¡å¼ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»º RAG å®ä¾‹
    rag = OilfieldRAG(data_dir=args.data_dir, config_path=args.config)
    rag.initialize()
    
    if args.question:
        # ç›´æ¥å›ç­”é—®é¢˜
        answer = rag.ask(args.question, verbose=True)
        print(f"\nğŸ’¬ ç­”æ¡ˆ:\n{answer}")
    elif args.mode == "web":
        run_web(rag)
    else:
        run_cli(rag)


if __name__ == "__main__":
    main()
