"""
æ–‡æœ¬å—æ‘˜è¦æ¨¡å— - ä¸ºæ¯ä¸ª chunk ç”Ÿæˆç²¾ç‚¼æ‘˜è¦

è´Ÿè´£äººï¼šæˆå‘˜Aï¼ˆæ•°æ®å·¥ç¨‹å¸ˆï¼‰+ æˆå‘˜Cï¼ˆPrompt å·¥ç¨‹å¸ˆï¼‰

åŠŸèƒ½ï¼š
1. ä¸ºæ–‡æœ¬å—ç”Ÿæˆç²¾ç‚¼æ‘˜è¦ï¼ˆLLMï¼‰
2. æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©ä¸åŒçš„æ‘˜è¦ç­–ç•¥
3. æ‘˜è¦å¯ç”¨äºå¢å¼ºæ£€ç´¢ï¼ˆåœ¨åŸæ–‡å‰æ·»åŠ ï¼‰
"""

import os
import sys
import logging
from typing import List, Dict, Optional
from langchain_core.documents import Document

logger = logging.getLogger(__name__)

# é»˜è®¤æ‘˜è¦ Prompt æ¨¡æ¿
DEFAULT_PROMPTS = {
    "excel_record": """Summarize this data record in ONE sentence.
Focus on: entity name, key metrics (with numbers), time period.
Be specific and include actual values.

Record: {content}

Summary (one sentence):""",

    "pdf_table_record": """Summarize this table data in ONE sentence.
Focus on: what data it contains, key values, entity names.
Include specific numbers if present.

Table data: {content}

Summary (one sentence):""",

    "ppt_slide": """Summarize this presentation slide in ONE sentence.
Focus on: main message, key data points.

Slide: {content}

Summary (one sentence):""",

    "contract_clause": """Summarize this contract clause in ONE sentence.
Focus on: what it defines, key terms, obligations.

Clause: {content}

Summary (one sentence):""",

    "image_caption": """Summarize what this image shows in ONE sentence.
Focus on: type of content, key information visible.

Image description: {content}

Summary (one sentence):""",

    "default": """Summarize this text in 2-3 sentences.
Focus on: main topic, key facts, important numbers or metrics.
Be concise and specific.

Text: {content}

Summary (2-3 sentences):"""
}


class ChunkSummarizer:
    """
    æ–‡æœ¬å—æ‘˜è¦ç”Ÿæˆå™¨
    
    ç­–ç•¥ï¼š
    1. çŸ­æ–‡æœ¬ï¼ˆ<300å­—ï¼‰ï¼šè·³è¿‡æ‘˜è¦ï¼Œç›´æ¥ç”¨åŸæ–‡
    2. ä¸­ç­‰æ–‡æœ¬ï¼ˆ300-2000å­—ï¼‰ï¼šç”Ÿæˆ 1-2 å¥æ‘˜è¦
    3. é•¿æ–‡æœ¬ï¼ˆ>2000å­—ï¼‰ï¼šç”Ÿæˆ 2-3 å¥ç»“æ„åŒ–æ‘˜è¦
    
    Example:
        summarizer = ChunkSummarizer(llm=my_llm)
        summarized_docs = summarizer.summarize(docs)
    """
    
    def __init__(
        self,
        llm=None,
        prompts: Dict[str, str] = None,
        min_length: int = 300,
        max_input_length: int = 3000,
        prepend_summary: bool = True,
        batch_size: int = 10
    ):
        """
        Args:
            llm: LangChain LLM å®ä¾‹
            prompts: è‡ªå®šä¹‰æ‘˜è¦ Promptï¼ˆæŒ‰æ–‡æ¡£ç±»å‹ï¼‰
            min_length: ä½äºæ­¤é•¿åº¦çš„æ–‡æ¡£è·³è¿‡æ‘˜è¦
            max_input_length: è¾“å…¥åˆ° LLM çš„æœ€å¤§é•¿åº¦
            prepend_summary: æ˜¯å¦å°†æ‘˜è¦æ·»åŠ åˆ°åŸæ–‡å¼€å¤´
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆé¢„ç•™ï¼Œç”¨äºå¹¶è¡Œï¼‰
        """
        self.llm = llm
        self.prompts = prompts or DEFAULT_PROMPTS
        self.min_length = min_length
        self.max_input_length = max_input_length
        self.prepend_summary = prepend_summary
        self.batch_size = batch_size
    
    def summarize(
        self,
        docs: List[Document],
        verbose: bool = True
    ) -> List[Document]:
        """
        ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆæ‘˜è¦
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            verbose: æ˜¯å¦æ‰“å°è¿›åº¦
            
        Returns:
            æ·»åŠ äº†æ‘˜è¦çš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.llm:
            logger.warning("No LLM provided, skipping summarization")
            return self._fallback_summarize(docs)
        
        summarized = []
        stats = {"total": 0, "skipped": 0, "llm_generated": 0, "fallback": 0}
        
        for i, doc in enumerate(docs):
            content_len = len(doc.page_content)
            stats["total"] += 1
            
            # çŸ­æ–‡æœ¬ï¼šè·³è¿‡
            if content_len < self.min_length:
                doc.metadata["summary"] = doc.page_content[:200]
                doc.metadata["summary_type"] = "skipped_short"
                summarized.append(doc)
                stats["skipped"] += 1
                continue
            
            # ä¸­é•¿æ–‡æœ¬ï¼šç”Ÿæˆæ‘˜è¦
            try:
                summary = self._generate_summary(doc)
                doc.metadata["summary"] = summary
                doc.metadata["summary_type"] = "llm_generated"
                
                # å¯é€‰ï¼šå°†æ‘˜è¦æ·»åŠ åˆ°åŸæ–‡å¼€å¤´ï¼ˆå¢å¼ºæ£€ç´¢ï¼‰
                if self.prepend_summary and summary:
                    doc.page_content = f"[Summary: {summary}]\n\n{doc.page_content}"
                
                stats["llm_generated"] += 1
                
            except Exception as e:
                logger.warning(f"Summarization failed for doc {i}: {e}")
                doc.metadata["summary"] = doc.page_content[:200]
                doc.metadata["summary_type"] = "fallback"
                stats["fallback"] += 1
            
            summarized.append(doc)
            
            # è¿›åº¦æ‰“å°
            if verbose and (i + 1) % 50 == 0:
                print(f"ğŸ“ Summarized {i + 1}/{len(docs)} chunks")
        
        if verbose:
            logger.info(f"Summarization complete: {stats['total']} docs")
            logger.info(f"  - Skipped (short): {stats['skipped']}")
            logger.info(f"  - LLM generated: {stats['llm_generated']}")
            logger.info(f"  - Fallback: {stats['fallback']}")
        
        return summarized
    
    def _generate_summary(self, doc: Document) -> str:
        """ç”Ÿæˆå•ä¸ªæ–‡æ¡£çš„æ‘˜è¦"""
        doc_type = doc.metadata.get("type", "default")
        content = doc.page_content[:self.max_input_length]
        
        # è·å–å¯¹åº”ç±»å‹çš„ Prompt
        prompt_template = self.prompts.get(doc_type, self.prompts["default"])
        prompt = prompt_template.format(content=content)
        
        # è°ƒç”¨ LLM
        response = self.llm.invoke(prompt)
        summary = response.content.strip()
        
        # æ¸…ç†æ‘˜è¦
        summary = self._clean_summary(summary)
        
        return summary[:500]  # é™åˆ¶æ‘˜è¦é•¿åº¦
    
    def _clean_summary(self, summary: str) -> str:
        """æ¸…ç†æ‘˜è¦æ–‡æœ¬"""
        # å»é™¤å¸¸è§çš„ LLM å‰ç¼€
        prefixes_to_remove = [
            "Summary:", "Here is the summary:", "The summary is:",
            "This text summarizes:", "In summary,",
            "æ‘˜è¦ï¼š", "æ€»ç»“ï¼š", "æœ¬æ–‡æ‘˜è¦ï¼š",
        ]
        
        for prefix in prefixes_to_remove:
            if summary.lower().startswith(prefix.lower()):
                summary = summary[len(prefix):].strip()
                break
        
        # å»é™¤å¼•å·
        if summary.startswith('"') and summary.endswith('"'):
            summary = summary[1:-1]
        
        return summary.strip()
    
    def _fallback_summarize(self, docs: List[Document]) -> List[Document]:
        """æ—  LLM æ—¶çš„å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨é¦–å¥/é¦–è¡Œä½œä¸ºæ‘˜è¦"""
        for doc in docs:
            content = doc.page_content
            
            # å°è¯•å–é¦–å¥
            import re
            sentences = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+', content[:500])
            if sentences:
                summary = sentences[0][:200]
            else:
                summary = content[:200]
            
            doc.metadata["summary"] = summary
            doc.metadata["summary_type"] = "no_llm_fallback"
        
        return docs


class CachedChunkSummarizer(ChunkSummarizer):
    """
    å¸¦ç¼“å­˜çš„æ‘˜è¦ç”Ÿæˆå™¨
    
    ä½¿ç”¨æ–‡æ¡£å†…å®¹çš„ hash ä½œä¸º keyï¼Œé¿å…é‡å¤ç”Ÿæˆæ‘˜è¦
    """
    
    def __init__(self, cache_dir: str = ".summary_cache", **kwargs):
        super().__init__(**kwargs)
        self.cache_dir = cache_dir
        self._cache = {}
        self._load_cache()
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        import json
        cache_file = os.path.join(self.cache_dir, "summaries.json")
        
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self._cache = json.load(f)
                logger.info(f"Loaded {len(self._cache)} cached summaries")
            except Exception as e:
                logger.warning(f"Failed to load cache: {e}")
                self._cache = {}
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        import json
        os.makedirs(self.cache_dir, exist_ok=True)
        cache_file = os.path.join(self.cache_dir, "summaries.json")
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self._cache, f, ensure_ascii=False, indent=2)
            logger.info(f"Saved {len(self._cache)} summaries to cache")
        except Exception as e:
            logger.warning(f"Failed to save cache: {e}")
    
    def _get_content_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹ hash"""
        import hashlib
        return hashlib.md5(content[:1000].encode()).hexdigest()[:16]
    
    def summarize(self, docs: List[Document], verbose: bool = True) -> List[Document]:
        """å¸¦ç¼“å­˜çš„æ‘˜è¦ç”Ÿæˆ"""
        if not self.llm:
            return self._fallback_summarize(docs)
        
        summarized = []
        stats = {"total": 0, "cached": 0, "new": 0}
        
        for i, doc in enumerate(docs):
            content_hash = self._get_content_hash(doc.page_content)
            stats["total"] += 1
            
            # æ£€æŸ¥ç¼“å­˜
            if content_hash in self._cache:
                summary = self._cache[content_hash]
                doc.metadata["summary"] = summary
                doc.metadata["summary_type"] = "cached"
                
                if self.prepend_summary and summary:
                    doc.page_content = f"[Summary: {summary}]\n\n{doc.page_content}"
                
                stats["cached"] += 1
            else:
                # ç”Ÿæˆæ–°æ‘˜è¦
                if len(doc.page_content) < self.min_length:
                    summary = doc.page_content[:200]
                    doc.metadata["summary_type"] = "skipped_short"
                else:
                    try:
                        summary = self._generate_summary(doc)
                        doc.metadata["summary_type"] = "llm_generated"
                        
                        if self.prepend_summary and summary:
                            doc.page_content = f"[Summary: {summary}]\n\n{doc.page_content}"
                    except Exception as e:
                        logger.warning(f"Summarization failed: {e}")
                        summary = doc.page_content[:200]
                        doc.metadata["summary_type"] = "fallback"
                
                doc.metadata["summary"] = summary
                self._cache[content_hash] = summary
                stats["new"] += 1
            
            summarized.append(doc)
            
            if verbose and (i + 1) % 50 == 0:
                print(f"ğŸ“ Summarized {i + 1}/{len(docs)} chunks")
        
        # ä¿å­˜ç¼“å­˜
        if stats["new"] > 0:
            self._save_cache()
        
        if verbose:
            logger.info(f"Summarization complete: {stats['total']} docs")
            logger.info(f"  - Cached: {stats['cached']}")
            logger.info(f"  - New: {stats['new']}")
        
        return summarized


def summarize_chunks(
    docs: List[Document],
    llm=None,
    use_cache: bool = True,
    verbose: bool = True
) -> List[Document]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¸ºæ–‡æ¡£ç”Ÿæˆæ‘˜è¦
    
    Args:
        docs: æ–‡æ¡£åˆ—è¡¨
        llm: LangChain LLM å®ä¾‹
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        verbose: æ˜¯å¦æ‰“å°è¿›åº¦
        
    Returns:
        æ·»åŠ äº†æ‘˜è¦çš„æ–‡æ¡£åˆ—è¡¨
    """
    if use_cache:
        summarizer = CachedChunkSummarizer(llm=llm)
    else:
        summarizer = ChunkSummarizer(llm=llm)
    
    return summarizer.summarize(docs, verbose=verbose)
