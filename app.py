#!/usr/bin/env python3
"""
ğŸ›¢ï¸ æ²¹ç”°æœåŠ¡ RAG ç³»ç»Ÿ - ä¸»åº”ç”¨å…¥å£

ä½¿ç”¨æ–¹æ³•:
    1. æŠŠç»„å§”ä¼šæ•°æ®æ”¾åˆ° data/ æ–‡ä»¶å¤¹
    2. è¿è¡Œ: python app.py

æˆ–è€…æŒ‡å®šæ•°æ®ç›®å½•:
    python app.py --data_dir /path/to/data

å¯åŠ¨ Web UI:
    python app.py --mode web
"""

import os
import sys
import argparse
import uuid
import yaml
from typing import List, Dict

# è®¾ç½®ç¯å¢ƒ
os.environ["NO_PROXY"] = "localhost,127.0.0.1"

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))


def load_config(config_path: str = "config/config.yaml") -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    default_config = {
        "data": {"root_dir": "data/"},
        "models": {
            "llm": {"model_name": "qwen2.5:3b", "base_url": "http://127.0.0.1:11434"},
            "embedding": {"model_name": "sentence-transformers/all-MiniLM-L6-v2"},
            "reranker": {"model_name": "cross-encoder/ms-marco-MiniLM-L-6-v2"}
        },
        "indexing": {
            "chunk_size_parent": 2000,
            "chunk_overlap_parent": 200,
            "chunk_size_child": 400,
            "chunk_overlap_child": 50
        }
    }
    
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            # åˆå¹¶é»˜è®¤é…ç½®
            for key in default_config:
                if key not in config:
                    config[key] = default_config[key]
            return config
    
    return default_config


class OilfieldRAG:
    """
    æ²¹ç”°æœåŠ¡ RAG ç³»ç»Ÿä¸»ç±»
    
    Example:
        rag = OilfieldRAG(data_dir="data/")
        answer = rag.ask("What is the revenue of SLB?")
    """
    
    def __init__(self, data_dir: str = "data/", config_path: str = "config/config.yaml"):
        self.data_dir = data_dir
        self.config = load_config(config_path)
        self.config_path = config_path
        
        self.vectorstore = None
        self.bm25 = None
        self.reranker = None
        self.llm = None
        self.retriever = None
        self.generator = None
        self.docstore = {}
        self.splits = []
        
        self._initialized = False
    
    def initialize(self, verbose: bool = True):
        """åˆå§‹åŒ–ç³»ç»Ÿï¼ˆåŠ è½½æ•°æ®ã€æ„å»ºç´¢å¼•ï¼‰"""
        if self._initialized:
            print("âš ï¸ ç³»ç»Ÿå·²åˆå§‹åŒ–")
            return
        
        print("=" * 60)
        print("ğŸ›¢ï¸ æ²¹ç”°æœåŠ¡ RAG ç³»ç»Ÿ åˆå§‹åŒ–")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        print("\nğŸ“‚ Step 1: åŠ è½½æ•°æ®...")
        from src.loaders import load_all_documents
        
        # æ£€æŸ¥ VLM
        vlm = self._init_vlm()
        
        docs = load_all_documents(self.data_dir, vlm=vlm, verbose=verbose)
        
        if not docs:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•")
            return
        
        # 2. æ–‡æ¡£åˆ‡åˆ† (Parent-Child)
        print("\nğŸ“‘ Step 2: æ–‡æ¡£åˆ‡åˆ†...")
        self.splits, self.docstore = self._split_documents(docs)
        print(f"   ç”Ÿæˆ {len(self.splits)} ä¸ªå­å—")
        
        # 3. æ„å»ºå‘é‡ç´¢å¼•
        print("\nğŸ” Step 3: æ„å»ºå‘é‡ç´¢å¼•...")
        self.vectorstore = self._build_vectorstore(self.splits)
        
        # 4. æ„å»º BM25 ç´¢å¼•
        print("\nğŸ“ Step 4: æ„å»º BM25 ç´¢å¼•...")
        self.bm25 = self._build_bm25(self.splits)
        
        # 5. åŠ è½½ Reranker
        print("\nğŸ¯ Step 5: åŠ è½½ Reranker...")
        self.reranker = self._load_reranker()
        
        # 6. åŠ è½½ LLM
        print("\nğŸ¤– Step 6: è¿æ¥ LLM...")
        self.llm = self._init_llm()
        
        # 7. åˆå§‹åŒ–æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨
        print("\nâš™ï¸ Step 7: åˆå§‹åŒ–æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨...")
        from src.retrieval import RAGRetriever
        from src.generation import create_generator
        
        self.retriever = RAGRetriever(
            vectorstore=self.vectorstore,
            bm25=self.bm25,
            splits=self.splits,
            reranker=self.reranker,
            docstore=self.docstore,
            llm=self.llm,
            config=self.config
        )
        
        self.generator = create_generator(self.llm, self.config_path)
        
        self._initialized = True
        
        print("\n" + "=" * 60)
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ!")
        print(f"   æ–‡æ¡£æ•°: {len(docs)}")
        print(f"   ç´¢å¼•å—: {len(self.splits)}")
        print(f"   LLM: {'åœ¨çº¿' if self.llm else 'ç¦»çº¿'}")
        print("=" * 60)
    
    def ask(self, question: str, verbose: bool = False) -> str:
        """
        é—®ç­”æ¥å£
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            verbose: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯
        
        Returns:
            ç­”æ¡ˆå­—ç¬¦ä¸²
        """
        if not self._initialized:
            self.initialize()
        
        if verbose:
            print(f"\nâ“ é—®é¢˜: {question}")
        
        # æ£€ç´¢
        docs, retrieval_debug = self.retriever.retrieve(question, top_k=5)
        
        if verbose:
            print(f"ğŸ“š æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
            for i, doc in enumerate(docs[:3], 1):
                print(f"   {i}. {doc.page_content[:80]}...")
        
        # ç”Ÿæˆ
        answer, gen_debug = self.generator.generate(question, docs)
        
        if verbose:
            print(f"\nğŸ’¬ ç­”æ¡ˆ: {answer[:200]}...")
        
        return answer
    
    def _split_documents(self, docs: List) -> tuple:
        """Parent-Child åˆ‡åˆ†"""
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        
        cfg = self.config["indexing"]
        parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=cfg["chunk_size_parent"],
            chunk_overlap=cfg["chunk_overlap_parent"]
        )
        child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=cfg["chunk_size_child"],
            chunk_overlap=cfg["chunk_overlap_child"]
        )
        
        docstore = {}
        child_docs = []
        
        for raw_doc in docs:
            # åˆ‡åˆ†ä¸º Parent
            if len(raw_doc.page_content) > cfg["chunk_size_parent"]:
                parents = parent_splitter.split_documents([raw_doc])
            else:
                parents = [raw_doc]
            
            for parent in parents:
                parent_id = str(uuid.uuid4())
                parent.metadata["doc_id"] = parent_id
                docstore[parent_id] = parent
                
                # åˆ‡åˆ†ä¸º Child
                children = child_splitter.split_documents([parent])
                for child in children:
                    child.metadata["parent_id"] = parent_id
                    child_docs.append(child)
        
        return child_docs, docstore
    
    def _build_vectorstore(self, docs: List):
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        from langchain_huggingface import HuggingFaceEmbeddings
        from langchain_chroma import Chroma
        
        model_name = self.config["models"]["embedding"]["model_name"]
        embeddings = HuggingFaceEmbeddings(model_name=model_name)
        
        return Chroma.from_documents(documents=docs, embedding=embeddings)
    
    def _build_bm25(self, docs: List):
        """æ„å»º BM25 ç´¢å¼•"""
        from rank_bm25 import BM25Okapi
        from src.text_processing import tokenize_text
        
        tokenized = [tokenize_text(doc.page_content) for doc in docs]
        return BM25Okapi(tokenized)
    
    def _load_reranker(self):
        """åŠ è½½ Reranker"""
        from sentence_transformers import CrossEncoder
        
        model_name = self.config["models"]["reranker"]["model_name"]
        return CrossEncoder(model_name)
    
    def _init_llm(self):
        """åˆå§‹åŒ– LLM"""
        try:
            from langchain_ollama import ChatOllama
            
            cfg = self.config["models"]["llm"]
            llm = ChatOllama(
                model=cfg["model_name"],
                base_url=cfg["base_url"]
            )
            llm.invoke("hi")  # æµ‹è¯•è¿æ¥
            print(f"   âœ… LLM è¿æ¥æˆåŠŸ: {cfg['model_name']}")
            return llm
        except Exception as e:
            print(f"   âš ï¸ LLM è¿æ¥å¤±è´¥: {e}")
            return None
    
    def _init_vlm(self):
        """åˆå§‹åŒ– VLM (å›¾ç‰‡ç†è§£)"""
        if not self.config.get("models", {}).get("vlm", {}).get("enabled", False):
            return None
        
        try:
            from langchain_ollama import ChatOllama
            
            cfg = self.config["models"]["vlm"]
            vlm = ChatOllama(
                model=cfg["model_name"],
                base_url=cfg["base_url"]
            )
            vlm.invoke("hi")
            print(f"   âœ… VLM å¯ç”¨: {cfg['model_name']}")
            return vlm
        except:
            print("   âš ï¸ VLM ä¸å¯ç”¨ï¼Œå›¾ç‰‡å°†ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæè¿°")
            return None


def run_cli(rag: OilfieldRAG):
    """å‘½ä»¤è¡Œäº¤äº’æ¨¡å¼"""
    print("\nğŸ® è¿›å…¥äº¤äº’æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
    print("-" * 40)
    
    while True:
        try:
            question = input("\nâ“ è¯·è¾“å…¥é—®é¢˜: ").strip()
            
            if question.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§!")
                break
            
            if not question:
                continue
            
            answer = rag.ask(question, verbose=True)
            print(f"\nğŸ’¬ ç­”æ¡ˆ:\n{answer}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break


def run_web(rag: OilfieldRAG):
    """å¯åŠ¨ Web UI"""
    print("\nğŸŒ å¯åŠ¨ Web UI...")
    print("   è¯·è®¿é—®: http://localhost:8501")
    
    # ä½¿ç”¨åŸæœ‰çš„ streamlit_app
    os.system("streamlit run streamlit_app.py")


def main():
    parser = argparse.ArgumentParser(description="æ²¹ç”°æœåŠ¡ RAG ç³»ç»Ÿ")
    parser.add_argument("--data_dir", type=str, default="data/", help="æ•°æ®ç›®å½•")
    parser.add_argument("--config", type=str, default="config/config.yaml", help="é…ç½®æ–‡ä»¶")
    parser.add_argument("--mode", type=str, choices=["cli", "web"], default="cli", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--question", type=str, help="ç›´æ¥æé—®ï¼ˆéäº¤äº’æ¨¡å¼ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»º RAG å®ä¾‹
    rag = OilfieldRAG(data_dir=args.data_dir, config_path=args.config)
    rag.initialize()
    
    if args.question:
        # ç›´æ¥å›ç­”é—®é¢˜
        answer = rag.ask(args.question, verbose=True)
        print(f"\nğŸ’¬ ç­”æ¡ˆ:\n{answer}")
    elif args.mode == "web":
        run_web(rag)
    else:
        run_cli(rag)


if __name__ == "__main__":
    main()
