# =====================================================
# ğŸ›¢ï¸ æ²¹ç”°æœåŠ¡ RAG ç³»ç»Ÿ - ç»Ÿä¸€é…ç½®æ–‡ä»¶
# =====================================================
# ä½¿ç”¨æ–¹æ³•ï¼šä¿®æ”¹æ­¤æ–‡ä»¶å³å¯è°ƒæ•´ç³»ç»Ÿè¡Œä¸ºï¼Œæ— éœ€æ”¹ä»£ç 

# -----------------------------------------------------
# ğŸ“‚ æ•°æ®è·¯å¾„é…ç½®
# -----------------------------------------------------
data:
  # æ•°æ®æ ¹ç›®å½•ï¼ˆæŠŠç»„å§”ä¼šçš„æ•°æ®æ”¾è¿™é‡Œï¼‰
  root_dir: "data/"
  
  # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
  supported_formats:
    documents: ["pdf", "docx", "xlsx", "csv", "md", "pptx"]
    images: ["png", "jpg", "jpeg"]
    videos: ["mp4"]  # æš‚ä¸æ”¯æŒ

# -----------------------------------------------------
# ğŸ¤– æ¨¡å‹é…ç½®
# -----------------------------------------------------
models:
  # LLM é…ç½®
  llm:
    provider: "azure_openai"  # ä½¿ç”¨ Azure OpenAI
    model_name: "gpt-5-chat"  # fallback å¦‚æœ azure_openai é…ç½®ä¸å­˜åœ¨
    base_url: "http://127.0.0.1:11434"  # Ollama fallback
    temperature: 0.1  # ä½æ¸©åº¦ = æ›´ç¡®å®šæ€§çš„å›ç­”
  
  # VLM é…ç½® (å›¾ç‰‡ç†è§£)
  vlm:
    enabled: false  # Azure OpenAI ä¸æ”¯æŒ VLM
    model_name: "llava"
    base_url: "http://127.0.0.1:11434"
  
  # Embedding æ¨¡å‹
  # å¯é€‰ provider: "azure_openai", "huggingface", "huggingface_local"
  embedding:
    provider: "azure_openai"  # ä½¿ç”¨ Azure OpenAI embedding
    model_name: "text-embedding-3-large"
    # æœ¬åœ° GPU æ¨¡å‹é…ç½®ï¼ˆprovider: "huggingface_local" æ—¶ä½¿ç”¨ï¼‰
    # model_name: "BAAI/bge-large-en-v1.5"
    # device: "cuda"  # æˆ– "cpu"

  # Azure OpenAI é…ç½®
  azure_openai:
    team_domain: ""  # ä» .env è¯»å– TEAM_DOMAIN
    api_key_env: "AZURE_OPENAI_API_KEY"
    completion_model: "gpt-5-chat"
    completion_model_fallback: "gpt-5-mini"
    embedding_model: "text-embedding-3-large"
  
  # Reranker æ¨¡å‹
  reranker:
    # è‹±æ–‡è¯­æ–™ä¼˜å…ˆï¼ˆè½»é‡çº§ Cross-Encoderï¼‰
    model_name: "cross-encoder/ms-marco-MiniLM-L-6-v2"

# -----------------------------------------------------
# ğŸ“‘ æ–‡æ¡£å¤„ç†é…ç½®
# -----------------------------------------------------
indexing:
  # Parent-Child åˆ†å—ç­–ç•¥ï¼ˆé»˜è®¤é…ç½®ï¼‰
  chunk_size_parent: 2000    # çˆ¶å—å¤§å°ï¼ˆç”¨äºç”Ÿæˆï¼‰
  chunk_overlap_parent: 200
  chunk_size_child: 400      # å­å—å¤§å°ï¼ˆç”¨äºæ£€ç´¢ï¼‰
  chunk_overlap_child: 50

# æ¯”èµ›ä¼˜åŒ–é…ç½®ï¼ˆå¤§æ•°æ®é‡æ—¶ä½¿ç”¨ï¼Œå‡å°‘ chunk æ•°é‡ï¼‰
indexing_competition:
  chunk_size_parent: 3000
  chunk_overlap_parent: 300
  chunk_size_child: 600
  chunk_overlap_child: 80

# -----------------------------------------------------
# ğŸ” æ£€ç´¢é…ç½®
# -----------------------------------------------------
retrieval:
  # åˆ†ç»„å¬å› Top-K
  k_excel: 10      # Excel/CSV æ•°æ®
  k_word: 5        # Word åˆåŒ
  k_image: 3       # å›¾ç‰‡æè¿°
  k_tech: 5        # æŠ€æœ¯æ–‡æ¡£ï¼ˆMarkdownï¼‰
  k_pdf_table: 6   # PDF è¡¨æ ¼è®°å½•
  k_pdf_text: 5    # PDF æ–‡æœ¬
  k_ppt: 5         # PPT å¹»ç¯ç‰‡
  k_general: 5     # é€šç”¨
  
  # è´¢åŠ¡é—®é¢˜åŠ¨æ€åŠ æƒ
  k_financial_boost: 15
  
  # RRF èåˆå‚æ•°
  rrf:
    k: 60          # RRF å¸¸æ•°
    weights:
      vector: 1.0
      bm25: 0.8
      hyde: 1.2
  
  # HyDE é…ç½®
  hyde:
    enabled: true

# -----------------------------------------------------
# ğŸ“ Prompt æ¨¡æ¿
# -----------------------------------------------------
prompts:
  # é—®é¢˜æ”¹å†™ (CRAG)
  rewrite: |
    You are a question re-writer that converts an input question to a better version optimized for vectorstore retrieval.
    Input Question: {original_query}
    Output ONLY the improved question. No other text.

  # æ–‡æ¡£è´¨é‡è¯„ä¼° (CRAG)
  grade: |
    You are a grader assessing relevance of a retrieved document to a user question.
    Question: {query}
    Document: {doc_content}
    Give a binary score 'yes' or 'no'. Output only 'yes' or 'no'.

  # ç­”æ¡ˆå¯ä¿¡æ€§æ ¡éªŒ (Guard)
  faithfulness_check: |
    You are a strict fact-checker.
    Question: {query}
    Answer: {answer}
    Context: {context}
    Is the answer fully supported by the context? Answer only 'yes' or 'no'.

  # æœ€ç»ˆå›ç­”ç”Ÿæˆï¼ˆå¸¦ Few-Shot ç¤ºä¾‹ï¼‰
  generation: |
    You are a Senior Technical Expert in the Oil & Gas industry.
    Answer the user's question directly and accurately based on the Context provided.
    
    CRITICAL INSTRUCTIONS:
    1. Answer in the SAME LANGUAGE as the question (ä¸­æ–‡é—®é¢˜ç”¨ä¸­æ–‡å›ç­”).
    2. Do NOT include citations or source-referencing language.
    3. If you cannot find the answer, respond in the same language:
       - English: "I don't know."
       - ä¸­æ–‡: "ä¸çŸ¥é“ã€‚"
    4. Keep your answer concise and direct.
    5. For numerical values, include units (e.g., "33.1 Billion USD", "18%", "1500 m").
    
    EXAMPLES:
    Q: What is SLB's 2023 revenue?
    A: SLB: 33.1 Billion USD
    
    Q: Which region has the highest Q3 2024 revenue?
    A: Middle East: 850.5 Million USD
    
    Q: What is the average porosity of Zone_A?
    A: 18%
    
    Q: ä¸­æµ·æ²¹2023å¹´å‡€åˆ©æ¶¦æ˜¯å¤šå°‘ï¼Ÿ
    A: 121.6 äº¿ç¾å…ƒ
    
    Context:
    {context}
    
    User Question: {query}
    
    Answer:

  # é€‰æ‹©é¢˜å›ç­”
  choice_answer: |
    You are answering a multiple-choice question.
    Answer format must be: "<OptionLetter>. <OptionText>".
    No explanation.
    
    Context:
    {context}
    
    Question:
    {query}
    
    Answer:

  # æ¯”è¾ƒç±»é—®é¢˜èšåˆ
  aggregation_compare: |
    Based on the retrieved information, provide a comparative analysis.
    
    Sub-questions and contexts:
    {sub_results}
    
    Original Question: {original_query}
    
    Instructions:
    1. Clearly state the values for each item being compared
    2. Use specific numbers from the context
    3. Highlight key differences
    
    Answer:

  # ç½®ä¿¡åº¦åˆ¤æ–­
  confidence_check: |
    Based on the following context, can you answer the question?
    
    Context:
    {context}
    
    Question: {query}
    
    Instructions:
    - Reply with ONLY a number between 0.0 and 1.0 indicating your confidence.
    - 0.0 = The context contains NO relevant information to answer the question.
    - 0.5 = The context contains SOME related information but not enough for a definitive answer.
    - 1.0 = The context contains ALL necessary information to answer the question with certainty.
    
    Confidence (0.0-1.0):

  # ç­”æ¡ˆæ ¸å¿ƒæå–ï¼ˆLLM åå¤„ç†ï¼Œæ›¿ä»£ hardcode æ­£åˆ™ï¼‰
  answer_extraction: |
    Extract ONLY the direct answer from the response below.
    
    Remove ALL of the following:
    - Greetings and pleasantries (ä½ å¥½, å¾ˆé«˜å…´, sure, certainly)
    - Citation language (æ ¹æ®æ–‡æ¡£, according to, based on, æ¥æº)
    - Explanations and reasoning (å› ä¸º, because, since, ç”±äº)
    - Filler words and disclaimers
    
    Rules:
    - If it's a choice question, format as: "X. <option text>" (e.g., A. æ²‰ç§¯å²©)
    - If it's a number, include the unit (e.g., 33.1 Billion USD)
    - If it says "don't know", output exactly: ä¸çŸ¥é“ã€‚
    - Keep the answer language same as the original
    
    Raw response: {raw_answer}
    Original question: {query}
    
    Direct answer only:

  # é€‰æ‹©é¢˜æ ¼å¼ä¿®æ­£ï¼ˆLLM æ™ºèƒ½æ ¼å¼åŒ–ï¼‰
  format_choice: |
    Given this answer and these options, output the correct choice in the exact format.
    
    Options:
    {options}
    
    Answer: {answer}
    
    Output format must be exactly: "<letter>. <full option text>"
    Example: A. æ²‰ç§¯å²©
    
    Do NOT add any explanation.
    
    Formatted answer:

  # æ ¸å¿ƒé—®é¢˜æå–ï¼ˆè¿‡æ»¤å¯’æš„ã€èƒŒæ™¯ã€åºŸè¯ï¼‰
  core_question_extraction: |
    Extract the core question from the user input.
    Remove ALL of the following:
    - Greetings (ä½ å¥½, æ‚¨å¥½, hello, hi, hey)
    - Politeness phrases (è¯·é—®, è°¢è°¢, thanks, please, could you)
    - Self-introductions (æˆ‘æ˜¯..., I'm..., my name is...)
    - Background rambling (å…³äºé‚£ä¸ª..., å°±æ˜¯é‚£ä¸ª...)
    - Irrelevant context
    
    Keep ONLY the actual question the user wants answered.
    Output the core question in the SAME LANGUAGE as the original.
    If the input is already a pure question, return it unchanged.
    
    User input: {query}
    
    Core question:

  # ç­”æ¡ˆ-é—®é¢˜å¯¹é½æ£€æŸ¥
  answer_alignment_check: |
    Check if the answer addresses the core question.
    
    Core question: {core_query}
    Answer: {answer}
    
    Does the answer directly address the question? Reply only 'yes' or 'no'.
    Verdict:

  # æŸ¥è¯¢æ™ºèƒ½æ”¹å†™ï¼ˆè§£å†³å…¬å¸åå¼‚è¯‘ã€æœ¯è¯­ä¸ä¸€è‡´ç­‰é—®é¢˜ï¼‰
  query_normalization: |
    Normalize this query for better search retrieval in an oilfield services knowledge base.
    
    Rules:
    1. Convert company names to standard English names AND keep Chinese equivalent:
       - å“ˆåˆ©ä¼¯é¡¿/å“ˆé‡Œä¼¯é¡¿ â†’ Halliburton (å“ˆé‡Œä¼¯é¡¿)
       - æ–¯ä¼¦è´è°¢/æ–½ä¼¦ä¼¯æ° â†’ SLB/Schlumberger (æ–¯ä¼¦è´è°¢)
       - ä¸­æµ·æ²¹æœ/ä¸­æµ·æ²¹ç”°æœåŠ¡ â†’ COSL (ä¸­æµ·æ²¹æœ)
       - è´å…‹ä¼‘æ–¯ â†’ Baker Hughes (è´å…‹ä¼‘æ–¯)
       - å¨å¾·ç¦ â†’ Weatherford (å¨å¾·ç¦)
    2. Normalize financial terms with both languages:
       - è¥æ”¶/æ”¶å…¥ â†’ revenue (è¥æ”¶)
       - åˆ©æ¶¦ â†’ profit (åˆ©æ¶¦)
       - å‘˜å·¥æ•° â†’ employees (å‘˜å·¥)
    3. Keep numbers, years, and quarters (Q1-Q4) as-is
    4. Output a single search-friendly query combining key terms in both languages
    
    Input: {query}
    
    Normalized query:

# -----------------------------------------------------
# ğŸ“¦ é¢„å¤„ç†é…ç½®ï¼ˆå…ƒæ•°æ®æ¸…æ´— & æ‘˜è¦ï¼‰
# -----------------------------------------------------
preprocessing:
  # å…ƒæ•°æ®æ¸…æ´—ï¼ˆè½»é‡çº§ï¼Œæ¨èå¼€å¯ï¼‰
  enable_metadata_cleaning: true
  
  # æ–‡æœ¬å—æ‘˜è¦ï¼ˆéœ€è¦ LLMï¼Œå¯é€‰ï¼‰
  enable_summarization: false  # æ¯”èµ›æ—¶å»ºè®®é¢„å…ˆç”Ÿæˆå¹¶ç¼“å­˜
  
  # æ‘˜è¦é…ç½®
  summarization:
    min_length: 300         # ä½äºæ­¤é•¿åº¦çš„æ–‡æ¡£è·³è¿‡æ‘˜è¦
    max_input_length: 3000  # è¾“å…¥åˆ° LLM çš„æœ€å¤§é•¿åº¦
    prepend_summary: true   # å°†æ‘˜è¦æ·»åŠ åˆ°åŸæ–‡å¼€å¤´ï¼ˆå¢å¼ºæ£€ç´¢ï¼‰
    use_cache: true         # ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤ç”Ÿæˆ
    cache_dir: ".summary_cache"
    
    # æŒ‰æ–‡æ¡£ç±»å‹çš„æ‘˜è¦ Promptï¼ˆæˆå‘˜C å¯è°ƒä¼˜ï¼‰
    prompts:
      excel_record: |
        Summarize this data record in ONE sentence.
        Focus on: entity name, key metrics (with numbers), time period.
        Record: {content}
        Summary:
      
      pdf_table_record: |
        Summarize this table data in ONE sentence.
        Focus on: what data it contains, key values.
        Table: {content}
        Summary:
      
      ppt_slide: |
        Summarize this slide in ONE sentence.
        Focus on: main message, key data points.
        Slide: {content}
        Summary:
      
      default: |
        Summarize this text in 2-3 sentences.
        Focus on: main topic, key facts, important numbers.
        Text: {content}
        Summary:

# -----------------------------------------------------
# ğŸ“Š è¯„æµ‹é…ç½®
# -----------------------------------------------------
evaluation:
  # è¯„æµ‹æ¨¡å¼
  use_llm_judge: true  # ä½¿ç”¨ LLM ä½œä¸ºè¯„åˆ¤
  
  # é€šè¿‡é˜ˆå€¼
  pass_threshold: 7    # æ»¡åˆ† 10 åˆ†ï¼Œ7 åˆ†åŠæ ¼

# -----------------------------------------------------
# ğŸ“¥ æ‰¹é‡é—®ç­”ï¼ˆExcelï¼‰
# -----------------------------------------------------
batch_answer:
  # è¾“å…¥/è¾“å‡º Excel è·¯å¾„ï¼ˆå¯ä»¥åœ¨å‘½ä»¤è¡Œè¦†ç›–ï¼‰
  input_path: "data/questions.xlsx"
  output_path: ""  # ç•™ç©ºåˆ™è¦†ç›–è¾“å…¥æ–‡ä»¶
  question_col: "question"
  answer_col: "answer"
  start_row: 0
  limit: null
  resume: true       # è·³è¿‡å·²æœ‰ç­”æ¡ˆçš„è¡Œ
  save_every: 5      # æ¯å¤„ç† N è¡Œä¿å­˜ä¸€æ¬¡ï¼ˆé˜²ä¸­æ–­ï¼‰

# -----------------------------------------------------
# ğŸ–¥ï¸ ç³»ç»Ÿé…ç½®
# -----------------------------------------------------
system:
  # æ—¥å¿—çº§åˆ«
  log_level: "INFO"
  
  # å‘é‡æ•°æ®åº“
  vector_db:
    persist_dir: ".cache/chroma_db"
    force_rebuild: false  # true = æ¯æ¬¡é‡å»ºç´¢å¼•
  
  # ç¼“å­˜
  cache:
    enabled: true
    dir: ".cache/"
