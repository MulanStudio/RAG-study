import os
import sys

# å¼ºåˆ¶ç»•è¿‡ä»£ç†ï¼Œè§£å†³ 502 é”™è¯¯
os.environ["NO_PROXY"] = "localhost,127.0.0.1"

import pandas as pd
from langchain_community.document_loaders import (
    TextLoader, 
    DirectoryLoader, 
    PyPDFLoader, 
    Docx2txtLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from sentence_transformers import CrossEncoder
from langchain_core.documents import Document
from langchain_ollama import ChatOllama

# ç›´æ¥ä½¿ç”¨ rank_bm25ï¼Œä¸ä¾èµ– LangChain çš„ BM25Retriever
from rank_bm25 import BM25Okapi

# æ£€æŸ¥ç¯å¢ƒå˜é‡
if "OPENAI_API_KEY" not in os.environ:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡ (LLM éƒ¨åˆ†å¯èƒ½æ— æ³•è¿è¡Œ)ã€‚")

def load_excel_as_text(directory):
    """
    ä¸“é—¨ä¼˜åŒ– Excel åŠ è½½ï¼šTable-to-Text
    """
    documents = []
    import glob
    excel_files = glob.glob(os.path.join(directory, "**/*.xlsx"), recursive=True)
    
    for file_path in excel_files:
        try:
            df = pd.read_excel(file_path)
            df = df.fillna("Unknown")
            for index, row in df.iterrows():
                content = (
                    f"Market Data Record: Company {row.get('Company', '')} "
                    f"is located in {row.get('Country', '')}. "
                    f"Its 2023 Revenue was {row.get('Revenue_2023_Billion_USD', '')} Billion USD. "
                    f"It has {row.get('Employees', '')} employees. "
                    f"Key service focus is {row.get('Key_Service', '')}."
                )
                doc = Document(
                    page_content=content, 
                    metadata={"source": file_path, "type": "excel_record"}
                )
                documents.append(doc)
            print(f"âœ… æˆåŠŸåŠ è½½ Excel (è½¬è‡ªç„¶è¯­è¨€): {file_path} (è¡Œæ•°: {len(df)})")
        except Exception as e:
            print(f"âš ï¸  Excel åŠ è½½å¤±è´¥ {file_path}: {e}")
            
    return documents

def load_word_with_structure(directory):
    """
    ä¸“é—¨ä¼˜åŒ– Word åŠ è½½ï¼šä¿ç•™æ–‡æ¡£ç»“æ„ (Title + Content)
    å¯¹äºåˆåŒï¼Œ"Section 2. Compensation" è¿™æ ·çš„æ ‡é¢˜å¯¹æ£€ç´¢è‡³å…³é‡è¦ã€‚
    """
    documents = []
    import glob
    from docx import Document as DocxDocument # éœ€è¦ pip install python-docx
    
    word_files = glob.glob(os.path.join(directory, "**/*.docx"), recursive=True)
    
    for file_path in word_files:
        try:
            doc_obj = DocxDocument(file_path)
            current_heading = "General"
            
            for para in doc_obj.paragraphs:
                text = para.text.strip()
                if not text:
                    continue
                
                # ç®€å•åˆ¤æ–­ï¼šå¦‚æœæ˜¯ç²—ä½“æˆ–è€…å­—æ•°è¾ƒå°‘ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜
                # è¿™é‡Œå‡è®¾ style.name åŒ…å« 'Heading' æ˜¯æ ‡é¢˜
                if 'Heading' in para.style.name:
                    current_heading = text
                else:
                    # å°†æ ‡é¢˜æ‹¼æ¥åˆ°å†…å®¹å‰ï¼Œå¢å¼ºè¯­ä¹‰
                    # ä¾‹å¦‚: "Section 2. Compensation: Client agrees to pay..."
                    enhanced_content = f"Document Section [{current_heading}]: {text}"
                    
                    doc = Document(
                        page_content=enhanced_content, 
                        metadata={"source": file_path, "type": "contract_clause", "section": current_heading}
                    )
                    documents.append(doc)
                    
            print(f"âœ… æˆåŠŸåŠ è½½ Word (ç»“æ„åŒ–): {file_path}")
        except Exception as e:
            print(f"âš ï¸  Word åŠ è½½å¤±è´¥ {file_path}: {e}")
            
    return documents

def run_rag_demo():
    print("--- 1. Data Processing (æ•°æ®å¤„ç†ä¼˜åŒ–ç‰ˆ) ---")
    
    docs = []
    
    # 1.1 åŠ è½½ Markdown (ä½¿ç”¨ MarkdownHeaderTextSplitter ä¼˜åŒ–)
    # ä¸å†ç”¨ TextLoader å‚»è¯»ï¼Œè€Œæ˜¯è¯»å–å†…å®¹åæŒ‰æ ‡é¢˜åˆ‡åˆ†
    import glob
    md_files = glob.glob(os.path.join("knowledge_base", "**/*.md"), recursive=True)
    for f in md_files:
        with open(f, 'r', encoding='utf-8') as file:
            md_content = file.read()
            
        # å®šä¹‰åˆ‡åˆ†è§„åˆ™ï¼šæŒ‰ H1, H2, H3 åˆ‡åˆ†ï¼Œä¿ç•™å±‚çº§ç»“æ„
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
        ]
        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
        md_header_splits = markdown_splitter.split_text(md_content)
        
        # ç»™æ¯ä¸ªåˆ‡åˆ†å‡ºæ¥çš„å—åŠ ä¸Š source å…ƒæ•°æ®
        for doc in md_header_splits:
            doc.metadata["source"] = f
            doc.metadata["type"] = "markdown_section"
            # è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼šæŠŠæ ‡é¢˜æ‹¼å›æ­£æ–‡ï¼Œå¢å¼ºè¯­ä¹‰
            # MarkdownHeaderTextSplitter ä¼šæŠŠæ ‡é¢˜æ”¾åœ¨ metadata é‡Œï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒæ‹¿å‡ºæ¥
            header_path = " > ".join([v for k, v in doc.metadata.items() if k.startswith("Header")])
            if header_path:
                doc.page_content = f"Section [{header_path}]:\n{doc.page_content}"
                
        docs.extend(md_header_splits)
        print(f"âœ… æˆåŠŸåŠ è½½ Markdown (æŒ‰æ ‡é¢˜åˆ‡åˆ†): {f} (å—æ•°: {len(md_header_splits)})")
    
    # 1.2 åŠ è½½ Word (æ”¹ä¸ºç»“æ„åŒ–åŠ è½½)
    # ç§»é™¤æ—§çš„ Docx2txtLoader
    # loader_word = DirectoryLoader("downloads", glob="**/*.docx", loader_cls=Docx2txtLoader)
    # docs.extend(loader_word.load())
    docs.extend(load_word_with_structure("downloads"))
    
    # 1.3 åŠ è½½ Excel (Table-to-Text)
    docs.extend(load_excel_as_text("downloads"))
    
    # 1.4 åŠ è½½ PDF
    print("æ­£åœ¨åŠ è½½ PDF...")
    pdf_loader = DirectoryLoader("downloads", glob="**/*.pdf", loader_cls=PyPDFLoader)
    docs.extend(pdf_loader.load())
    
    print(f"æ€»è®¡åŠ è½½æ–‡æ¡£ç‰‡æ®µ: {len(docs)}")

    # 1.5 åˆ‡åˆ†æ–‡æ¡£ (ä»…åˆ‡åˆ† PDF å’Œå…¶ä»–é•¿æ–‡æœ¬ï¼ŒMarkdown/Excel/Word å·²ç»åˆ‡å¥½äº†)
    # è¿™é‡Œçš„é€»è¾‘éœ€è¦è°ƒæ•´ï¼šæˆ‘ä»¬åªå¯¹é‚£äº›å¤ªé•¿çš„ doc è¿›è¡ŒäºŒæ¬¡åˆ‡åˆ†
    final_docs = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    
    for doc in docs:
        # å¦‚æœæ˜¯ PDF (type="pdf" æˆ–æœªæ ‡è®°)ï¼Œåˆ‡åˆ†
        # å¦‚æœæ˜¯ Markdown/Excel/Word (å·²ç»ç»“æ„åŒ–å¤„ç†è¿‡)ï¼Œå¦‚æœå¤ªé•¿ä¹Ÿåˆ‡ä¸€ä¸‹ï¼Œä½†å°½é‡ä¿ç•™å®Œæ•´
        if len(doc.page_content) > 1000:
             final_docs.extend(text_splitter.split_documents([doc]))
        else:
             final_docs.append(doc)
             
    splits = final_docs
    print(f"æ–‡æ¡£é¢„å¤„ç†å®Œæˆï¼Œå…± {len(splits)} ä¸ª Chunksã€‚")

    print("\n--- 2. Building Retrievers (æ‰‹åŠ¨æ„å»ºæ··åˆæ£€ç´¢) ---")
    
    # 2.1 å‘é‡æ£€ç´¢ (Chroma)
    print("åˆå§‹åŒ– Vector Store...")
    # å…³é”®ä¿®æ”¹ï¼šåˆ‡æ¢åˆ°æ”¯æŒå¤šè¯­è¨€çš„ Embedding æ¨¡å‹
    # paraphrase-multilingual-MiniLM-L12-v2: è½»é‡çº§ä½†æ”¯æŒ50+ç§è¯­è¨€ï¼Œä¸­è‹±æ–‡å¯¹é½æ•ˆæœè¿œå¥½äº all-MiniLM
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    
    # 2.2 å…³é”®è¯æ£€ç´¢ (BM25)
    print("åˆå§‹åŒ– BM25 Index...")
    # å¯¹æ‰€æœ‰ chunk è¿›è¡Œåˆ†è¯
    tokenized_corpus = [doc.page_content.split(" ") for doc in splits]
    bm25 = BM25Okapi(tokenized_corpus)

    # 2.3 Reranker
    print("åŠ è½½ Reranker æ¨¡å‹...")
    reranker = CrossEncoder("BAAI/bge-reranker-base")
    
    # 2.4 åˆå§‹åŒ–æœ¬åœ° LLM (Ollama)
    print("åˆå§‹åŒ–æœ¬åœ° LLM (Ollama)...")
    try:
        # ä½¿ç”¨ qwen2.5:3bï¼Œæ˜¾å¼æŒ‡å®š URL
        llm = ChatOllama(
            model="qwen2.5:3b",
            base_url="http://127.0.0.1:11434"
        )
        # æµ‹è¯•ä¸€ä¸‹ LLM æ˜¯å¦å¯ç”¨
        print("   -> æµ‹è¯• LLM è¿æ¥...")
        llm.invoke("Hi")
        print("   -> LLM è¿æ¥æˆåŠŸï¼")
        use_llm = True
    except Exception as e:
        print(f"âš ï¸  æœ¬åœ° LLM åˆå§‹åŒ–å¤±è´¥: {e}")
        print("   -> å°†ä»…è¿”å›æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µï¼Œä¸è¿›è¡Œç”Ÿæˆã€‚")
        use_llm = False

    print("\n--- 3. Execution (æ‰§è¡Œæ£€ç´¢ + ç”Ÿæˆ) ---")
    
    questions = [
        "Schlumberger çš„ 2023 å¹´è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ(Excelæ•°æ®)",
        "åˆåŒä¸­è§„å®šçš„é’»äº•æ—¥è´¹ç‡(day rate)æ˜¯å¤šå°‘ï¼Ÿ(Wordæ•°æ®)",
        "ä»€ä¹ˆæ˜¯æ°´åŠ›å‹è£‚(Fracking)ï¼Ÿ(MarkdownçŸ¥è¯†åº“)",
    ]

    for q in questions:
        print(f"\n" + "="*50)
        print(f"ç”¨æˆ·æé—®: {q}")
        print("-" * 50)
        
        # --- Step 1: æ··åˆå¬å› (Hybrid Retrieval) - é‡‡ç”¨åˆ†ç»„å¬å›ç­–ç•¥ (Grouped Retrieval) ---
        print("ğŸ” 1. åˆ†ç»„æ··åˆå¬å› (Grouped Retrieval)...")
        
        candidate_docs = []
        seen_content = set()
        
        # å®šä¹‰åˆ†ç»„ç­–ç•¥ï¼šå¼ºåˆ¶ä»æ¯ç§æ–‡æ¡£ç±»å‹é‡Œéƒ½æä¸€ç‚¹å‡ºæ¥
        # è¿™æ ·å°±èƒ½é¿å…å¤§æ–‡ä»¶ (PDF) æ·¹æ²¡å°æ–‡ä»¶ (Excel/Word)
        filters = [
            {"name": "Excel", "filter": {"type": "excel_record"}, "k": 5},
            {"name": "Word", "filter": {"type": "contract_clause"}, "k": 5},
            {"name": "General", "filter": None, "k": 10} # General è´Ÿè´£æ PDF å’Œå…¶ä»–æœªåˆ†ç±»çš„
        ]
        
        for f in filters:
            kwargs = {"k": f["k"]}
            if f["filter"]:
                kwargs["filter"] = f["filter"]
                
            try:
                sub_docs = vectorstore.similarity_search(q, **kwargs)
                print(f"   - [{f['name']}] å‘é‡å¬å›: {len(sub_docs)} ä¸ª")
                
                for doc in sub_docs:
                    if doc.page_content not in seen_content:
                        # ç®€å•çš„æ„å›¾åˆ¤æ–­åŠ æˆ
                        if f["name"].lower() in q.lower() or (f["name"]=="Word" and "åˆåŒ" in q):
                             doc.metadata["boost"] = True
                        candidate_docs.append(doc)
                        seen_content.add(doc.page_content)
            except Exception as e:
                print(f"   - [{f['name']}] å¬å›å¤±è´¥: {e}")

        # BM25 è¡¥å…… (å…³é”®è¯åŒ¹é…ï¼Œé˜²æ­¢å‘é‡æ¨¡å‹â€œåç§‘â€)
        tokenized_query = q.split(" ")
        bm25_top_n = bm25.get_top_n(tokenized_query, splits, n=10)
        for doc in bm25_top_n:
            if doc.page_content not in seen_content:
                candidate_docs.append(doc)
                seen_content.add(doc.page_content)
                
        print(f"   -> å€™é€‰æ–‡æ¡£æ€»æ•°: {len(candidate_docs)}")

        # --- Step 2: é‡æ’åº (Rerank) ---
        print("ğŸ” 2. é‡æ’åº (Reranking)...")
        pairs = [[q, doc.page_content] for doc in candidate_docs]
        scores = reranker.predict(pairs)
        
        doc_score_pairs = list(zip(candidate_docs, scores))
        
        final_pairs = []
        for doc, score in doc_score_pairs:
            final_score = score
            if doc.metadata.get("boost"):
                print(f"   [Boost] å‘ç°ç”¨æˆ·æ„å›¾åŒ¹é…æ–‡æ¡£: {os.path.basename(doc.metadata.get('source', ''))}")
                final_score += 0.5 
            final_pairs.append((doc, final_score))
            
        final_pairs.sort(key=lambda x: x[1], reverse=True)
        
        # é€‰å‡º Top 3
        top_docs = [doc for doc, score in final_pairs[:3]]
        
        # å±•ç¤ºæ£€ç´¢ç»“æœ
        for i, (doc, score) in enumerate(final_pairs[:3]):
            print(f"\n[æ’å #{i+1} | å¾—åˆ†: {score:.4f}]:")
            short_source = os.path.basename(doc.metadata.get('source', 'unknown'))
            print(f"æ¥æº: {short_source}")
            print(f"å†…å®¹: {doc.page_content[:200].replace(chr(10), ' ')}...")
            
        # --- Step 3: LLM ç”Ÿæˆ (Generation) ---
        if use_llm:
            print("\nğŸ¤– 3. LLM ç”Ÿæˆå›ç­”...")
            context = "\n\n".join([d.page_content for d in top_docs])
            prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚

å‚è€ƒèµ„æ–™:
{context}

é—®é¢˜: {q}

å›ç­”:"""
            response = llm.invoke(prompt)
            print(f"\nâœ… AI å›ç­”:\n{response.content}")
        else:
            print("\n(è·³è¿‡ç”Ÿæˆæ­¥éª¤ï¼Œä»…å±•ç¤ºæ£€ç´¢ç»“æœ)")

    print("\nâœ… æ¼”ç¤ºç»“æŸã€‚")

if __name__ == "__main__":
    run_rag_demo()
